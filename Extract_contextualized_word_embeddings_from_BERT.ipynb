{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a833162-2cc8-45df-91e5-1f47a7727606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d63bc02b-884a-4eaf-bbd4-ab3652eb8255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(sentence,model,tokenizer,layers=[-4, -3, -2, -1]):\n",
    "        \n",
    "    encoded_sentence= tokenizer.encode_plus(sentence,return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_sentence)\n",
    "        \n",
    "    hidden_states = output.hidden_states\n",
    "    \n",
    "    output = torch.stack([hidden_states[i] for i in layers]).sum(0).squeeze()\n",
    "    \n",
    "    sentence=sentence.strip().split(\" \")\n",
    "    \n",
    "    word_embeddings={}\n",
    "    \n",
    "    for word_index in range(len(sentence)):\n",
    "        tokens_for_word_id = np.where(np.array(encoded_sentence.word_ids()) == word_index)\n",
    "        word_tokens = output[tokens_for_word_id]\n",
    "        word_embeddings[sentence[word_index]]=word_tokens.mean(dim=0)\n",
    "    \n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcb4c7d9-1b40-49cf-afaf-4d334fc7faa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "sentence =\"this is a simple example on how to extract contextualized word embeddings from BERT\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-cased\", output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afe61c3e-db71-4717-9f18-189e1b2ee188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>this</th>\n",
       "      <th>is</th>\n",
       "      <th>a</th>\n",
       "      <th>simple</th>\n",
       "      <th>example</th>\n",
       "      <th>on</th>\n",
       "      <th>how</th>\n",
       "      <th>to</th>\n",
       "      <th>extract</th>\n",
       "      <th>contextualized</th>\n",
       "      <th>word</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>from</th>\n",
       "      <th>BERT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.630641</td>\n",
       "      <td>1.787610</td>\n",
       "      <td>0.753495</td>\n",
       "      <td>2.547040</td>\n",
       "      <td>-1.056210</td>\n",
       "      <td>-2.126183</td>\n",
       "      <td>0.320213</td>\n",
       "      <td>3.557543</td>\n",
       "      <td>6.466884</td>\n",
       "      <td>4.723215</td>\n",
       "      <td>2.969266</td>\n",
       "      <td>4.320745</td>\n",
       "      <td>2.094373</td>\n",
       "      <td>-0.355209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.258968</td>\n",
       "      <td>-0.165800</td>\n",
       "      <td>0.170220</td>\n",
       "      <td>0.700909</td>\n",
       "      <td>-1.225829</td>\n",
       "      <td>-1.486979</td>\n",
       "      <td>-2.497697</td>\n",
       "      <td>0.120161</td>\n",
       "      <td>0.303110</td>\n",
       "      <td>-0.011509</td>\n",
       "      <td>3.591769</td>\n",
       "      <td>3.829406</td>\n",
       "      <td>2.694431</td>\n",
       "      <td>-0.765758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.651023</td>\n",
       "      <td>3.086640</td>\n",
       "      <td>-0.404416</td>\n",
       "      <td>-0.260927</td>\n",
       "      <td>-3.786962</td>\n",
       "      <td>-1.136896</td>\n",
       "      <td>-0.490549</td>\n",
       "      <td>-2.265811</td>\n",
       "      <td>-2.068238</td>\n",
       "      <td>-0.995770</td>\n",
       "      <td>2.217768</td>\n",
       "      <td>-0.862766</td>\n",
       "      <td>3.068004</td>\n",
       "      <td>-0.993397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.337982</td>\n",
       "      <td>0.871563</td>\n",
       "      <td>0.323382</td>\n",
       "      <td>4.165645</td>\n",
       "      <td>0.171016</td>\n",
       "      <td>-1.365587</td>\n",
       "      <td>1.145535</td>\n",
       "      <td>2.383952</td>\n",
       "      <td>-1.065548</td>\n",
       "      <td>-0.060255</td>\n",
       "      <td>0.949844</td>\n",
       "      <td>-0.793853</td>\n",
       "      <td>-1.799135</td>\n",
       "      <td>1.006229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.971201</td>\n",
       "      <td>0.120285</td>\n",
       "      <td>0.240250</td>\n",
       "      <td>-2.331689</td>\n",
       "      <td>-7.664180</td>\n",
       "      <td>0.233749</td>\n",
       "      <td>-1.454072</td>\n",
       "      <td>-1.417325</td>\n",
       "      <td>0.052033</td>\n",
       "      <td>-1.007352</td>\n",
       "      <td>0.203638</td>\n",
       "      <td>-0.725703</td>\n",
       "      <td>1.268796</td>\n",
       "      <td>-2.467154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>-5.255383</td>\n",
       "      <td>-0.034469</td>\n",
       "      <td>2.124026</td>\n",
       "      <td>-3.197527</td>\n",
       "      <td>-1.442207</td>\n",
       "      <td>-0.233140</td>\n",
       "      <td>0.564277</td>\n",
       "      <td>1.046748</td>\n",
       "      <td>-2.018145</td>\n",
       "      <td>-1.885844</td>\n",
       "      <td>-0.099606</td>\n",
       "      <td>-1.626473</td>\n",
       "      <td>-0.844005</td>\n",
       "      <td>-3.023158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>-2.037163</td>\n",
       "      <td>-0.218114</td>\n",
       "      <td>0.629302</td>\n",
       "      <td>4.901371</td>\n",
       "      <td>4.873151</td>\n",
       "      <td>-2.875613</td>\n",
       "      <td>-3.444485</td>\n",
       "      <td>-1.458117</td>\n",
       "      <td>1.384867</td>\n",
       "      <td>-2.920961</td>\n",
       "      <td>-3.455287</td>\n",
       "      <td>0.114403</td>\n",
       "      <td>-4.447982</td>\n",
       "      <td>-1.153096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>4.384311</td>\n",
       "      <td>3.310249</td>\n",
       "      <td>4.281366</td>\n",
       "      <td>4.679168</td>\n",
       "      <td>4.127016</td>\n",
       "      <td>-0.161553</td>\n",
       "      <td>-0.413020</td>\n",
       "      <td>4.250200</td>\n",
       "      <td>5.344140</td>\n",
       "      <td>1.910372</td>\n",
       "      <td>2.727714</td>\n",
       "      <td>-0.515223</td>\n",
       "      <td>2.061347</td>\n",
       "      <td>3.227530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>-1.041151</td>\n",
       "      <td>0.651437</td>\n",
       "      <td>2.766595</td>\n",
       "      <td>-0.317495</td>\n",
       "      <td>-2.777891</td>\n",
       "      <td>-0.887264</td>\n",
       "      <td>-2.311985</td>\n",
       "      <td>-0.770246</td>\n",
       "      <td>-4.797533</td>\n",
       "      <td>-0.092786</td>\n",
       "      <td>0.209306</td>\n",
       "      <td>0.315204</td>\n",
       "      <td>0.626048</td>\n",
       "      <td>0.653203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>3.319721</td>\n",
       "      <td>2.763595</td>\n",
       "      <td>2.002032</td>\n",
       "      <td>2.957751</td>\n",
       "      <td>1.643189</td>\n",
       "      <td>1.449914</td>\n",
       "      <td>2.034983</td>\n",
       "      <td>2.127559</td>\n",
       "      <td>0.540805</td>\n",
       "      <td>1.100555</td>\n",
       "      <td>1.735024</td>\n",
       "      <td>-0.107610</td>\n",
       "      <td>0.666286</td>\n",
       "      <td>0.610074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         this        is         a    simple   example        on       how  \\\n",
       "0   -1.630641  1.787610  0.753495  2.547040 -1.056210 -2.126183  0.320213   \n",
       "1   -0.258968 -0.165800  0.170220  0.700909 -1.225829 -1.486979 -2.497697   \n",
       "2    0.651023  3.086640 -0.404416 -0.260927 -3.786962 -1.136896 -0.490549   \n",
       "3   -0.337982  0.871563  0.323382  4.165645  0.171016 -1.365587  1.145535   \n",
       "4   -0.971201  0.120285  0.240250 -2.331689 -7.664180  0.233749 -1.454072   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "763 -5.255383 -0.034469  2.124026 -3.197527 -1.442207 -0.233140  0.564277   \n",
       "764 -2.037163 -0.218114  0.629302  4.901371  4.873151 -2.875613 -3.444485   \n",
       "765  4.384311  3.310249  4.281366  4.679168  4.127016 -0.161553 -0.413020   \n",
       "766 -1.041151  0.651437  2.766595 -0.317495 -2.777891 -0.887264 -2.311985   \n",
       "767  3.319721  2.763595  2.002032  2.957751  1.643189  1.449914  2.034983   \n",
       "\n",
       "           to   extract  contextualized      word  embeddings      from  \\\n",
       "0    3.557543  6.466884        4.723215  2.969266    4.320745  2.094373   \n",
       "1    0.120161  0.303110       -0.011509  3.591769    3.829406  2.694431   \n",
       "2   -2.265811 -2.068238       -0.995770  2.217768   -0.862766  3.068004   \n",
       "3    2.383952 -1.065548       -0.060255  0.949844   -0.793853 -1.799135   \n",
       "4   -1.417325  0.052033       -1.007352  0.203638   -0.725703  1.268796   \n",
       "..        ...       ...             ...       ...         ...       ...   \n",
       "763  1.046748 -2.018145       -1.885844 -0.099606   -1.626473 -0.844005   \n",
       "764 -1.458117  1.384867       -2.920961 -3.455287    0.114403 -4.447982   \n",
       "765  4.250200  5.344140        1.910372  2.727714   -0.515223  2.061347   \n",
       "766 -0.770246 -4.797533       -0.092786  0.209306    0.315204  0.626048   \n",
       "767  2.127559  0.540805        1.100555  1.735024   -0.107610  0.666286   \n",
       "\n",
       "         BERT  \n",
       "0   -0.355209  \n",
       "1   -0.765758  \n",
       "2   -0.993397  \n",
       "3    1.006229  \n",
       "4   -2.467154  \n",
       "..        ...  \n",
       "763 -3.023158  \n",
       "764 -1.153096  \n",
       "765  3.227530  \n",
       "766  0.653203  \n",
       "767  0.610074  \n",
       "\n",
       "[768 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings=get_word_embeddings(sentence,model,tokenizer)\n",
    "pd.DataFrame(word_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
